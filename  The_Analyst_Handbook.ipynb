{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a2bff7",
   "metadata": {},
   "source": [
    "#  Part 0: Project Setup & Data Generation\n",
    "*Before we start analyzing, let's generate a realistic \"E-Commerce\" dataset containing sales, dates, and customer info. This ensures the notebook runs reproducibly without needing external files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d650f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! 'ecommerce_data.csv' has been generated with 100 rows.\n",
      "   We are ready to start the comparison below!\n"
     ]
    }
   ],
   "source": [
    "# Python Setup: Generating the Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Define the dataset parameters\n",
    "np.random.seed(42) # Ensures we get the same random numbers every time\n",
    "row_count = 100\n",
    "dates = pd.date_range(start='2024-01-01', periods=row_count)\n",
    "categories = ['Electronics', 'Clothing', 'Home', 'Toys']\n",
    "cities = ['Berlin', 'Munich', 'Hamburg', 'Cologne']\n",
    "\n",
    "# 2. Create the DataFrame\n",
    "data = {\n",
    "    'Date': np.random.choice(dates, row_count),\n",
    "    'Order_ID': range(1001, 1001 + row_count),\n",
    "    'Category': np.random.choice(categories, row_count),\n",
    "    'Sales': np.random.randint(50, 500, row_count),\n",
    "    'Quantity': np.random.randint(1, 10, row_count),\n",
    "    'City': np.random.choice(cities, row_count)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3. Introduce \"Messy\" Data (Real World Simulation)\n",
    "# Adding NULL values to 'Category' to practice cleaning later\n",
    "df.loc[0:4, 'Category'] = np.nan \n",
    "\n",
    "# 4. Save to CSV\n",
    "csv_name = 'ecommerce_data.csv'\n",
    "df.to_csv(csv_name, index=False)\n",
    "\n",
    "print(f\"âœ… Success! '{csv_name}' has been generated with {row_count} rows.\")\n",
    "print(\"   We are ready to start the comparison below!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4f42b",
   "metadata": {},
   "source": [
    "# Part 1: Data Exploration & Filtering\n",
    "*Goal: Getting to know your datasetâ€”peeking, slicing, and dicing.*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **View Data (First Look)** | Open file | `SELECT * FROM table LIMIT 5;` | `df.head(5)` |\n",
    "| **Select Columns** | Click Header | `SELECT City, Sales FROM table;` | `df[['City', 'Sales']]` |\n",
    "| **Basic Filtering** | Filter > 'Berlin' | `WHERE City = 'Berlin'` | `df[df['City'] == 'Berlin']` |\n",
    "| **Multiple Filters (AND)** | Filter A & Filter B | `WHERE City = 'Berlin' AND Sales > 100` | `df[(df['City'] == 'Berlin') & (df['Sales'] > 100)]` |\n",
    "| **Sorting** | Sort Z to A | `ORDER BY Sales DESC` | `df.sort_values(by='Sales', ascending=False)` |\n",
    "| **Unique Values** | Remove Duplicates | `SELECT DISTINCT City FROM table;` | `df['City'].unique()` |\n",
    "| **Count Rows** | Count Status Bar | `SELECT COUNT(*) FROM table;` | `len(df)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5184aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. First 5 Rows of Data ---\n",
      "         Date  Order_ID Category  Sales  Quantity     City\n",
      "0  2024-02-21      1001      NaN     94         4   Berlin\n",
      "1  2024-04-02      1002      NaN    111         3  Cologne\n",
      "2  2024-01-15      1003      NaN    490         3  Hamburg\n",
      "3  2024-03-12      1004      NaN    183         3  Cologne\n",
      "4  2024-03-01      1005      NaN    333         4   Munich\n",
      "\n",
      "--- 2. High Value Orders in Berlin: 12 orders found ---\n",
      "          Date  Order_ID Category  Sales  Quantity    City\n",
      "25  2024-03-16      1026     Home    408         5  Berlin\n",
      "27  2024-01-22      1028     Home    460         7  Berlin\n",
      "33  2024-04-01      1034     Toys    283         4  Berlin\n",
      "\n",
      "--- 3. Top 3 Highest Quantity Orders ---\n",
      "          Date     City  Quantity  Sales\n",
      "55  2024-01-09  Cologne         9    236\n",
      "14  2024-01-22   Munich         9    280\n",
      "49  2024-02-08   Berlin         9    436\n"
     ]
    }
   ],
   "source": [
    "# Python Real Example: Exploring our E-Commerce Data\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# 2. Peek at the data (First 5 rows)\n",
    "print(\"--- 1. First 5 Rows of Data ---\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Filter: Find all orders from 'Berlin' with Sales over 200\n",
    "berlin_high_sales = df[ (df['City'] == 'Berlin') & (df['Sales'] > 200) ]\n",
    "print(f\"\\n--- 2. High Value Orders in Berlin: {len(berlin_high_sales)} orders found ---\")\n",
    "print(berlin_high_sales.head(3))\n",
    "\n",
    "# 4. Sort: Who bought the most items (Quantity)?\n",
    "sorted_data = df.sort_values(by='Quantity', ascending=False)\n",
    "print(\"\\n--- 3. Top 3 Highest Quantity Orders ---\")\n",
    "print(sorted_data[['Date', 'City', 'Quantity', 'Sales']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f8b22e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (130716354.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m-- SQL Equivalent\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-- SQL Equivalent\n",
    "If we were running this on a database, here is the syntax for the operations above:\n",
    "\n",
    "-- 1. View Data (First 5 rows)\n",
    "SELECT * FROM orders \n",
    "LIMIT 5;\n",
    "\n",
    "-- 2. Filter (Berlin & Sales > 200)\n",
    "SELECT * FROM orders \n",
    "WHERE City = 'Berlin' AND Sales > 200;\n",
    "\n",
    "-- 3. Sort by Quantity (Highest first)\n",
    "SELECT Date, City, Quantity, Sales \n",
    "FROM orders \n",
    "ORDER BY Quantity DESC \n",
    "LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e82cf",
   "metadata": {},
   "source": [
    "# Part 2: Data Aggregation & Grouping\n",
    "*Goal: Moving from detailed rows to summary statistics (The \"Pivot Table\" logic).*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Sum per Group** | Pivot Table (Rows: City, Values: Sum of Sales) | `SELECT City, SUM(Sales) FROM table GROUP BY City;` | `df.groupby('City')['Sales'].sum()` |\n",
    "| **Count per Group** | Pivot Table (Rows: Category, Values: Count of ID) | `SELECT Category, COUNT(*) FROM table GROUP BY Category;` | `df.groupby('Category')['Order_ID'].count()` |\n",
    "| **Average** | Pivot Table (Values: Average of Sales) | `SELECT AVG(Sales) FROM table;` | `df['Sales'].mean()` |\n",
    "| **Multiple Aggregations** | Pivot Table (Values: Sum & Avg) | `SELECT SUM(Sales), AVG(Price) ...` | `df.groupby('City').agg({'Sales': 'sum', 'Price': 'mean'})` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810aa6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Aggregating Data\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Data (Bu satÄ±rÄ± ekledik ki hata vermesin)\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# 2. Total Sales per City (Which city makes the most money?)\n",
    "# We use .to_frame() to make it look prettier in the output\n",
    "city_sales = df.groupby('City')['Sales'].sum().sort_values(ascending=False)\n",
    "print(\"--- 1. Total Revenue by City ---\")\n",
    "display(city_sales.to_frame())\n",
    "\n",
    "# 3. Category Performance (Avg Price & Total Quantity Sold)\n",
    "category_stats = df.groupby('Category').agg({\n",
    "    'Sales': 'mean',      # Average Price\n",
    "    'Quantity': 'sum'     # Total Volume\n",
    "})\n",
    "print(\"\\n--- 2. Category Performance ---\")\n",
    "display(category_stats)\n",
    "\n",
    "# 4. Simple Stat: Overall Average Order Value\n",
    "avg_order = df['Sales'].mean()\n",
    "print(f\"\\n--- 3. Overall Average Order Value: ${avg_order:.2f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a191a2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent\n",
    "\n",
    "-- 1. Total Revenue by City\n",
    "SELECT City, SUM(Sales) as Total_Revenue\n",
    "FROM orders\n",
    "GROUP BY City\n",
    "ORDER BY Total_Revenue DESC;\n",
    "\n",
    "-- 2. Category Performance\n",
    "SELECT \n",
    "    Category, \n",
    "    AVG(Sales) as Avg_Sale_Price, \n",
    "    SUM(Quantity) as Total_Quantity\n",
    "FROM orders\n",
    "GROUP BY Category;\n",
    "\n",
    "-- 3. Overall Average\n",
    "SELECT AVG(Sales) FROM orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a1ea2",
   "metadata": {},
   "source": [
    "# Part 3: Data Merging (Joins)\n",
    "*Goal: Combining two tables based on a common column (The \"VLOOKUP\" logic).*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Left Join** (Keep all Sales, bring Manager) | `VLOOKUP` or `XLOOKUP` | `LEFT JOIN` | `pd.merge(df1, df2, on='Key', how='left')` |\n",
    "| **Inner Join** (Only matches) | Hard to do directly | `INNER JOIN` | `pd.merge(..., how='inner')` |\n",
    "| **Full Join** (Keep everything) | Not possible directly | `FULL OUTER JOIN` | `pd.merge(..., how='outer')` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Merging Data\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Main Data (Sales)\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# 2. Create a Second Dataset (Simulating a 'Managers' table)\n",
    "# Notice: We have 'Frankfurt' here, but no 'Cologne'.\n",
    "manager_data = {\n",
    "    'City': ['Berlin', 'Munich', 'Hamburg', 'Frankfurt'], \n",
    "    'Region': ['North', 'South', 'North', 'West'],\n",
    "    'Manager': ['Hans', 'Helga', 'Klaus', 'Petra']\n",
    "}\n",
    "df_managers = pd.DataFrame(manager_data)\n",
    "\n",
    "print(\"--- Secondary Table (Managers) ---\")\n",
    "display(df_managers)\n",
    "\n",
    "# 3. LEFT JOIN: Keep all Sales data, add Manager info where possible\n",
    "# This is exactly like doing a VLOOKUP in Excel.\n",
    "merged_df = pd.merge(df, df_managers, on='City', how='left')\n",
    "\n",
    "print(\"\\n--- Sales Data Combined with Manager Info (First 5 rows) ---\")\n",
    "display(merged_df[['Date', 'City', 'Sales', 'Manager', 'Region']].head())\n",
    "\n",
    "# 4. Check for Missing Values (The \"Real Case\" Scenario)\n",
    "# Since 'Cologne' is in Sales but NOT in Managers table, it should have NaN (Empty) values.\n",
    "missing_manager = merged_df[merged_df['Manager'].isnull()]\n",
    "print(f\"\\n--- Rows with Missing Manager (Cologne): {len(missing_manager)} rows ---\")\n",
    "display(missing_manager[['City', 'Sales', 'Manager']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e729a2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent\n",
    "\n",
    "-- Joining Sales table (orders) with Managers table\n",
    "-- We use LEFT JOIN to make sure we don't lose any sales records, even if a manager is missing.\n",
    "\n",
    "SELECT \n",
    "    orders.Date,\n",
    "    orders.City,\n",
    "    orders.Sales,\n",
    "    managers.Manager,\n",
    "    managers.Region\n",
    "FROM orders\n",
    "LEFT JOIN managers ON orders.City = managers.City;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0aa52",
   "metadata": {},
   "source": [
    "# Part 4: Data Cleaning & Text Manipulation\n",
    "*Goal: Fixing messy dataâ€”handling missing values (NULLs) and standardizing text.*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Check Missing** | Filter > Blanks | `WHERE Col IS NULL` | `df.isnull().sum()` |\n",
    "| **Fill Missing** | Find & Select > Go To Special > Blanks | `COALESCE(Col, 'Value')` | `df['Col'].fillna('Value')` |\n",
    "| **Drop Missing** | Delete Rows | `DELETE FROM table WHERE...` | `df.dropna()` |\n",
    "| **Text Replace** | `SUBSTITUTE(Cell, \"Old\", \"New\")` | `REPLACE(Col, 'Old', 'New')` | `df['Col'].str.replace('Old', 'New')` |\n",
    "| **Text Upper** | `UPPER(Cell)` | `UPPER(Col)` | `df['Col'].str.upper()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7260194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Cleaning Data\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# 2. Identify Missing Values (The \"Dirty\" Reality)\n",
    "# We expect to see missing values in 'Category' because we created them that way.\n",
    "print(\"--- 1. Count of Missing Values per Column ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Fill Missing Values (Imputation)\n",
    "# Logic: If Category is empty, assume it is 'Other'\n",
    "df['Category'] = df['Category'].fillna('Other')\n",
    "\n",
    "print(\"\\n--- 2. Verify Cleaning (First 5 rows - look at Category) ---\")\n",
    "# Notice row 0 to 4 might have been NaN, now they should be 'Other'\n",
    "display(df.head())\n",
    "\n",
    "# 4. Text Manipulation\n",
    "# Logic: Let's standardize 'City' names to be all UPPERCASE to avoid duplicates like 'Berlin' vs 'berlin'\n",
    "df['City'] = df['City'].str.upper()\n",
    "\n",
    "print(\"\\n--- 3. Standardized Text (Cities are now UPPERCASE) ---\")\n",
    "display(df[['Date', 'City', 'Sales']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143efe7e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--  SQL Equivalent\n",
    "\n",
    "-- 1. Find rows with missing Category\n",
    "SELECT * FROM orders \n",
    "WHERE Category IS NULL;\n",
    "\n",
    "-- 2. Fill Missing Values (Display logic, not permanent change)\n",
    "SELECT \n",
    "    Order_ID, \n",
    "    COALESCE(Category, 'Other') as Cleaned_Category \n",
    "FROM orders;\n",
    "\n",
    "-- 3. Make City Uppercase\n",
    "SELECT \n",
    "    Order_ID, \n",
    "    UPPER(City) as City_Upper \n",
    "FROM orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f66292",
   "metadata": {},
   "source": [
    "# Part 5: Date & Time Logic\n",
    "*Goal: Time travelâ€”extracting months, years, and calculating duration.*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Convert to Date** | Change Cell Format | `CAST(col AS DATE)` | `pd.to_datetime(df['col'])` |\n",
    "| **Extract Year** | `YEAR(Cell)` | `EXTRACT(YEAR FROM col)` | `df['col'].dt.year` |\n",
    "| **Extract Month** | `MONTH(Cell)` | `EXTRACT(MONTH FROM col)` | `df['col'].dt.month` |\n",
    "| **Day Difference** | `=Today - DateCell` | `CURRENT_DATE - col` | `(now - df['col']).dt.days` |\n",
    "| **Filter by Date** | Filter > Date Filters | `WHERE col >= '2024-01-01'` | `df[df['col'] >= '2024-01-01']` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbda656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Working with Dates\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# âš ï¸ CRITICAL STEP: Convert 'Date' column from String (Text) to Datetime Object\n",
    "# If you skip this, .dt accessor will not work!\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 2. Extract Year and Month (For Reporting)\n",
    "# We create new columns to analyze seasonality\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month_Name'] = df['Date'].dt.month_name()\n",
    "\n",
    "print(\"--- 1. Data with new Year/Month Columns ---\")\n",
    "display(df[['Date', 'Year', 'Month_Name', 'Sales']].head())\n",
    "\n",
    "# 3. Calculate \"Days Since Purchase\" (Recency)\n",
    "# How many days have passed since the order? (Crucial for identifying churn)\n",
    "today = pd.Timestamp.now()\n",
    "df['Days_Since'] = (today - df['Date']).dt.days\n",
    "\n",
    "print(\"\\n--- 2. Days Since Transaction (Recency) ---\")\n",
    "display(df[['Date', 'Days_Since']].head())\n",
    "\n",
    "# 4. Analysis: Total Sales by Month (Seasonality)\n",
    "# Now we can answer: \"Which month is our best month?\"\n",
    "monthly_sales = df.groupby('Month_Name')['Sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- 3. Best Performing Months ---\")\n",
    "display(monthly_sales.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86c334",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent\n",
    "\n",
    "-- 1. Extract parts of the date\n",
    "SELECT \n",
    "    Date,\n",
    "    EXTRACT(YEAR FROM Date) as Order_Year,\n",
    "    EXTRACT(MONTH FROM Date) as Order_Month\n",
    "FROM orders;\n",
    "\n",
    "-- 2. Calculate Age (Days Since)\n",
    "-- Note: Syntax varies by SQL type (PostgreSQL uses AGE or simply subtraction)\n",
    "SELECT \n",
    "    Date,\n",
    "    CURRENT_DATE - Date as Days_Since\n",
    "FROM orders;\n",
    "\n",
    "-- 3. Monthly Sales Analysis\n",
    "SELECT \n",
    "    EXTRACT(MONTH FROM Date) as Month, \n",
    "    SUM(Sales) as Total_Sales\n",
    "FROM orders\n",
    "GROUP BY EXTRACT(MONTH FROM Date)\n",
    "ORDER BY Total_Sales DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1029a7c",
   "metadata": {},
   "source": [
    "# Part 6: Conditional Logic (If/Else)\n",
    "*Goal: Decision makingâ€”creating new categories based on rules.*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas/Numpy) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Simple IF** (2 Choices) | `IF(A2>100, \"High\", \"Low\")` | `CASE WHEN Sales > 100 THEN 'High' ELSE 'Low' END` | `np.where(df['Sales'] > 100, 'High', 'Low')` |\n",
    "| **Multiple Conditions** (3+ Choices) | `IFS(A2>100,\"High\", A2>50,\"Med\", ...)` | `CASE WHEN... WHEN... ELSE... END` | `np.select([cond1, cond2], ['High', 'Med'], default='Low')` |\n",
    "| **Complex Logic** (Custom Function) | VBA Macro | Stored Procedure | `df.apply(lambda x: my_func(x), axis=1)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Customer Segmentation\n",
    "import pandas as pd\n",
    "import numpy as np  # <--- Essential for logic (np.where)\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# 2. Simple Logic: \"High Value\" Flag\n",
    "# If Sales > 300, it is a High Value Order, otherwise 'Standard'\n",
    "df['Order_Type'] = np.where(df['Sales'] > 300, 'High Value', 'Standard')\n",
    "\n",
    "print(\"--- 1. Simple Logic (High Value vs Standard) ---\")\n",
    "display(df[['Order_ID', 'Sales', 'Order_Type']].head())\n",
    "\n",
    "# 3. Multiple Conditions: Customer Tiers (Gold, Silver, Bronze)\n",
    "# Logic:\n",
    "#   - Sales > 400  -> 'Gold'\n",
    "#   - Sales > 200  -> 'Silver'\n",
    "#   - Else         -> 'Bronze'\n",
    "\n",
    "conditions = [\n",
    "    (df['Sales'] > 400),\n",
    "    (df['Sales'] > 200)\n",
    "]\n",
    "choices = ['Gold', 'Silver']\n",
    "\n",
    "# np.select checks conditions in order. If none match, it uses 'default'\n",
    "df['Tier'] = np.select(conditions, choices, default='Bronze')\n",
    "\n",
    "print(\"\\n--- 2. Advanced Logic (Gold/Silver/Bronze Tiers) ---\")\n",
    "display(df[['Order_ID', 'Sales', 'Tier']].head(10))\n",
    "\n",
    "# 4. Analysis: How many orders do we have per Tier?\n",
    "print(\"\\n--- 3. Tier Distribution ---\")\n",
    "display(df['Tier'].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b1a9f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent\n",
    "\n",
    "-- 1. Simple Case\n",
    "SELECT \n",
    "    Order_ID, \n",
    "    Sales,\n",
    "    CASE \n",
    "        WHEN Sales > 300 THEN 'High Value'\n",
    "        ELSE 'Standard'\n",
    "    END as Order_Type\n",
    "FROM orders;\n",
    "\n",
    "-- 2. Multiple Conditions (Tiers)\n",
    "SELECT \n",
    "    Order_ID, \n",
    "    Sales,\n",
    "    CASE \n",
    "        WHEN Sales > 400 THEN 'Gold'\n",
    "        WHEN Sales > 200 THEN 'Silver'\n",
    "        ELSE 'Bronze'\n",
    "    END as Tier\n",
    "FROM orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590c2d3",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Part 7: Statistics & Ranking (Window Functions)\n",
    "*Goal: Advanced Analysisâ€”Running totals, rankings, and comparing rows (e.g., Day vs Previous Day).*\n",
    "\n",
    "| Task | ðŸŸ¢ Excel | ðŸ”µ SQL | ðŸŸ¡ Python (Pandas) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Running Total** (Cumulative) | Pivot: Show Values As > Running Total | `SUM(col) OVER (ORDER BY date)` | `df['col'].cumsum()` |\n",
    "| **Ranking** (Leaderboard) | `RANK.EQ(cell, range)` | `RANK() OVER (ORDER BY col DESC)` | `df['col'].rank(ascending=False)` |\n",
    "| **Lag / Shift** (Prev Row) | `=A3 - A2` | `LAG(col) OVER (ORDER BY date)` | `df['col'].shift(1)` |\n",
    "| **Std Deviation** | `STDEV.S(range)` | `STDDEV(col)` | `df['col'].std()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac055dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Real Example: Advanced Stats & Window Functions\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. Load Data & Ensure Date is sorted (Crucial for Running Totals)\n",
    "df = pd.read_csv('ecommerce_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date') # Sort chronologically first!\n",
    "\n",
    "# 2. Running Total (Cumulative Sum)\n",
    "# Logic: \"How much total revenue have we made up to this specific date?\"\n",
    "df['Cumulative_Sales'] = df['Sales'].cumsum()\n",
    "\n",
    "print(\"--- 1. Running Total (Watch the Cumulative Column Grow) ---\")\n",
    "display(df[['Date', 'Sales', 'Cumulative_Sales']].head())\n",
    "\n",
    "# 3. Ranking\n",
    "# Logic: Rank orders from highest sales (1) to lowest.\n",
    "df['Rank'] = df['Sales'].rank(ascending=False)\n",
    "\n",
    "print(\"\\n--- 2. Ranking (Top 3 Orders) ---\")\n",
    "display(df.sort_values('Rank').head(3)[['Date', 'Sales', 'Rank']])\n",
    "\n",
    "# 4. Lag / Shift (Day-over-Day Growth)\n",
    "# Logic: Compare current row's sales with the PREVIOUS row's sales.\n",
    "df['Previous_Sales'] = df['Sales'].shift(1) # Moves everything down by 1\n",
    "df['Growth'] = df['Sales'] - df['Previous_Sales']\n",
    "\n",
    "print(\"\\n--- 3. Previous Row Comparison (Lag) ---\")\n",
    "display(df[['Date', 'Sales', 'Previous_Sales', 'Growth']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010ef8c",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent (Window Functions)\n",
    "\n",
    "-- 1. Running Total\n",
    "SELECT \n",
    "    Date, \n",
    "    Sales, \n",
    "    SUM(Sales) OVER (ORDER BY Date) as Cumulative_Sales\n",
    "FROM orders;\n",
    "\n",
    "-- 2. Ranking\n",
    "SELECT \n",
    "    Date, \n",
    "    Sales, \n",
    "    RANK() OVER (ORDER BY Sales DESC) as Sales_Rank\n",
    "FROM orders;\n",
    "\n",
    "-- 3. Lag (Previous Row)\n",
    "SELECT \n",
    "    Date, \n",
    "    Sales,\n",
    "    LAG(Sales) OVER (ORDER BY Date) as Previous_Sales\n",
    "FROM orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e291b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion & Key Takeaways\n",
    "\n",
    "In this project, we simulated a complete Data Analysis lifecycleâ€”from data generation to advanced statistical rankingâ€”demonstrating how business logic translates across the three most essential tools in the industry.\n",
    "\n",
    "### ðŸš€ Competencies Demonstrated:\n",
    "1.  **Tool Agnosticism:** Ability to seamlessly switch between **Excel** (Logic), **SQL** (Querying), and **Python** (Automation) depending on the business need.\n",
    "2.  **End-to-End Analysis:** Handled the full pipeline:\n",
    "    * *Exploration:* Slicing and filtering raw data.\n",
    "    * *Cleaning:* Handling missing values (`NULLs`) and standardizing text.\n",
    "    * *Modeling:* Joining disparate datasets (Sales + Managers).\n",
    "    * *Intelligence:* Creating customer tiers and analyzing time-series trends.\n",
    "3.  **Technical Depth:** Utilized advanced techniques like **Window Functions** and **Vectorized Operations** (NumPy) for efficient processing.\n",
    "\n",
    "### ðŸ’¼ Business Impact:\n",
    "This handbook serves not just as a code repository, but as a proof of concept that technical skills can be directly applied to solve business problems like **Churn Prediction**, **Sales Forecasting**, and **Customer Segmentation**.\n",
    "\n",
    "---\n",
    "*Thank you for reviewing my \"Analyst's Rosetta Stone\". Feel free to check out my other repositories or contact me for collaboration!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
